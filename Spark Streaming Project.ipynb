{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d591fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "# spark=SparkSession.builder.appName('streaming project').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbaa3a",
   "metadata": {},
   "source": [
    "## 1. Create a SparkContext with two execution threads, and StreamingContext with batch interval of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df811ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"StreamingProj\") #----------> to be executed only 1 time\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832bda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eefc25",
   "metadata": {},
   "source": [
    "## 2. Read and cache previous max price into an RDD. Each record of the RDD is a line of text with fields delimited by comma. We need to split the text line at the delimiter into individual strings of stock symbol and previous max price. Convert the previous max stock price to float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c8c61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JAH', '60.46'],\n",
       " ['JAS', '38.06'],\n",
       " ['JBI', '28.7'],\n",
       " ['JBI', '28.7'],\n",
       " ['JBJ', '26.8']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = sc.textFile(r'C:\\Users\\Miles\\Documents\\BigData\\Spark\\SPARK STREAMING MINI PROJECT\\previous_max_price.csv')\n",
    "\n",
    "prevRDD = prev.map(lambda x: x.split(',')) #.map(lambda x : [x[0], float(x[1])])\n",
    "prevRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2498ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JAH', 60.46], ['JAS', 38.06], ['JBI', 28.7], ['JBI', 28.7], ['JBJ', 26.8]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevPairedRDD = prevRDD.map(lambda x: [x[0] , float(x[1])])\n",
    "prevPairedRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb231b1",
   "metadata": {},
   "source": [
    "## 3. Using spark streaming context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999). This DStream represents the stream of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0eda79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_stock = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51ac72",
   "metadata": {},
   "source": [
    "## 4. Each record in this DStream is a line of text with fields delimited by comma. We will split the test line at the delimiter into individual strings. Convert each record into record to key-value pair with key being the stock symbol and value being a tuple of time stamp string and current price of float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c52c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_stock = sc.textFile(r'C:\\Users\\Miles\\Documents\\BigData\\Spark\\SPARK STREAMING MINI PROJECT\\stock_stream_data.csv')\n",
    "curr_stock_paired = curr_stock.map(lambda x: x.split(\",\")).map(lambda x: [x[0], (x[1], float(x[2]))])\n",
    "# curr_stock_paired.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feae383",
   "metadata": {},
   "source": [
    "## 5. We need to join data stream records with those of the previous max RDD on stock symbol field. We can use the join function, but since it is an RDD-to-RDD function we need to use transform method on the stream so that we can apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63623e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = curr_stock_paired.transform(lambda rdd: rdd.join(prevPairedRDD))\n",
    "# joined = curr_stock_paired.join(prevPairedRDD)\n",
    "# joined.take(5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ebab7",
   "metadata": {},
   "source": [
    "## 6. Joined records will be of the form key-value: stock_symbol, ((timestamp,currentprice),previous max price). So, record[0] will be stock_symbol, record[1] willhave 2 fields. In the two fields of record[1] - one is record[1][0] which has (time spamp,current price); the other is record[1][1] which has previous max price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeda17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(joined[0][1][0])     #------------> getting 2nd element in record \n",
    "# joined_paired.take(3)\n",
    "# joined_paired.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76584e9e",
   "metadata": {},
   "source": [
    "## 7. We need to filter for those records in which current price is greater than or equal to previous max price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_currPrice = joined.filter(lambda x: x[1][0][1]  >= x[1][1])\n",
    "# filter_currPrice = joined_paired.map(lambda x: x[1][1])\n",
    "# filter_currPrice.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc2d5a",
   "metadata": {},
   "source": [
    "## 8. Then we will use pprint (pretty print) function to print record matching the filter condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717fe2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_currPrice.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc90803",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdf6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0350215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed385e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
